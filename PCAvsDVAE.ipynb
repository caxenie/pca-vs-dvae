{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Principal Component Analysis vs. Denoising Variational Autoencoders\n",
    "## PCA vs. DVAE with examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "jupyter nbconvert *.ipynb --to slides --post serve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# An intuitive perspective ...\n",
    "\n",
    "*\"... natural, real-world high-dimensional data concentrates close to a non-linear low-dimensional manifold ...\"*\n",
    "\n",
    "![](manifold2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](manifold1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](manifold3.png)\n",
    "\n",
    "** But, how to learn the manifold and the probability distribution on it ? **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PCA\n",
    "* unsupervised learning\n",
    "* linear transformation that transforms a set of observations to a new coordinate system in which the values of the first coordinate have the largest possible variance [2]\n",
    "* computing the eigen decomposition of the covariance matrix\n",
    "* computing the singular value decomposition of the observations\n",
    "* decorrelation of the coordinates \n",
    "* reconstructions of the observations from the leading principal components have the least total squared error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Basic math of PCA \n",
    "\n",
    "Let $\\{y_i\\}^N_{i=1}$ be a set of $N$ observations vectors, each of size $n$, with $n\\leq N$. \n",
    "\n",
    "\n",
    "Let $Y \\in R^{nxN}$ be a matrix obtained by horizontally concatenating $\\{y_i\\}^N_{i=1}$, \n",
    "\n",
    "$ Y = \\begin{bmatrix} | ... | \\\\ y_1 ... y_N \\\\ | ... | \\end{bmatrix} $ \n",
    "\n",
    "We want to center the data to understand its statistical properties better, so we compute the element-wise mean of the $N$ observations as a $n$ dimensional vector \n",
    "\n",
    "$ \\bar{y} = \\frac{1}{N} \\sum_{i=1}^{N} {y_i} = \\frac{1}{N} Y 1_{N}$, \n",
    "\n",
    "where $1_N$ is a column vector of all-ones. We can calculate the centered observations as \n",
    "\n",
    "$Y_0 = Y - \\hat{y} 1_{N}^T $\n",
    "\n",
    "A linear transformation on a finite dimensional vector can be expressed as a matrix multiplication: \n",
    "\n",
    "$x_i = W y_i$, \n",
    "\n",
    "where $y_i \\in R^{n}, x_i \\in R^{m} and W \\in R^{nxm}$. Each $j-th$ element in $x_i$ is the inner product between $y_i$ and the $j-th$ column in $W$, denoted as $w_j$.\n",
    "\n",
    "Given the linear transformation, it is clear that \n",
    "\n",
    "$X = W^TY$ and $X_0 = W^TY_0$.\n",
    "\n",
    "In particular, when $W^T$ represents the transformation applying Principal Component Analysis, we denote $W = P$. Each column of $P$, denoted $\\{p_j\\}^n_{j=1}$ is a loading vector, whereas each transformed vector $\\{x_i\\}^N_{i=1}$ is a principal component.\n",
    "\n",
    "The first loading vector is the unit vector with which the inner products of the observations have the greatest variance:\n",
    "$p_1 = \\max w_1^T Y_0Y_0^Tw_1$ subject to $w_1^Tw_1 = 1$.\n",
    "\n",
    "The solution of the previous equation is the first eigenvector of the sample covariance matrix $Y_0Y_0^T$ corresponding to the largest eigenvalue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Autoencoders\n",
    "* unsupervised neural network\n",
    "* minimize the error of reconstructions of observations [1]\n",
    "* \n",
    "\n",
    "https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html\n",
    "\n",
    "\n",
    "# PCA vs. Autoencoders\n",
    "*  an autoencoder with a single fully-connected hidden layer, a linear activation function and a squared error cost function is closely related to PCA - its weights span the principal subspace [3]\n",
    "* \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Variational Autoencoders\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Denoising Variational Autoencoders\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense\n",
    "from keras import regularizers, models, optimizers\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Analytical PCA of the training set\n",
    "def AnalyticalPCA(y, dimension):\n",
    "    pca = PCA(n_components=dimension)\n",
    "    pca.fit(y)\n",
    "    loadings = pca.components_\n",
    "    return loadings\n",
    "\n",
    "# Linear Autoencoder\n",
    "def LinearAE(y, dimension, learning_rate = 1e-4, regularization = 5e-4, epochs=3):\n",
    "    input = Input(shape=(y.shape[1],))\n",
    "    encoded = Dense(dimension, activation='linear',\n",
    "                    kernel_regularizer=regularizers.l2(regularization))(input)\n",
    "    decoded = Dense(y.shape[1], activation='linear',\n",
    "                    kernel_regularizer=regularizers.l2(regularization))(encoded)\n",
    "    autoencoder = models.Model(input, decoded)\n",
    "    autoencoder.compile(optimizer=optimizers.adam(lr=learning_rate), loss='mean_squared_error')\n",
    "    autoencoder.fit(y, y, epochs=epochs, batch_size=4, shuffle=True)\n",
    "    (w1,b1,w2,b2)=autoencoder.get_weights()\n",
    "    return (w1,b1,w2,b2)\n",
    "\n",
    "def PlotResults(p,dimension,name):\n",
    "    sqrt_dimension = int(np.ceil(np.sqrt(dimension)))\n",
    "    plt.figure()\n",
    "    for i in range(p.shape[0]):\n",
    "        plt.subplot(sqrt_dimension, sqrt_dimension, i + 1)\n",
    "        plt.imshow(p[i, :, :],cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.savefig(name + '.png')\n",
    "\n",
    "dimension = 16                                                                  # feel free to change this, but you may have to tune hyperparameters\n",
    "(y, _), (_, _) = mnist.load_data(path='./mnist.npz')                                              # load MNIST training images\n",
    "\n",
    "shape_y = y.shape                                                               # store shape of y before reshaping it\n",
    "y = np.reshape(y,[shape_y[0],shape_y[1]*shape_y[2]]).astype('float32')/255      # reshape y to be a 2D matrix of the dataset\n",
    "p_analytical = AnalyticalPCA(y,dimension)                                       # PCA by applying SVD to y\n",
    "(_, _, w2, _) = LinearAE(y, dimension)                                          # train a linear autoencoder\n",
    "(p_linear_ae, _, _) = np.linalg.svd(w2.T, full_matrices=False)                    # PCA by applying SVD to linear autoencoder weights\n",
    "p_analytical = np.reshape(p_analytical,[dimension,shape_y[1],shape_y[2]])       # reshape loading vectors before plotting\n",
    "w2 = np.reshape(w2,[dimension,shape_y[1],shape_y[2]])                         # reshape autoencoder weights before plotting\n",
    "p_linear_ae = np.reshape(p_linear_ae.T, [dimension, shape_y[1], shape_y[2]])    # reshape loading vectors before plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PlotResults(p_analytical,dimension,'AnalyticalPCA')\n",
    "PlotResults(w2,dimension,'W2')\n",
    "PlotResults(p_linear_ae,dimension,'LinearAE_PCA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References and further reading\n",
    "[1] Ian Goodfellow, Yoshua Bengio and Aaron Courville, Deep Learning, MIT Press, 2016.\n",
    "\n",
    "[2] Jerome H. Friedman, Robert Tibshirani, and Trevor Hastie, The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Springer, 2017.\n",
    "\n",
    "[3] Plaut, E., 2018. From principal subspaces to principal components with linear autoencoders. arXiv preprint arXiv:1804.10253.\n",
    "\n",
    "[4] Im, D.I.J., Ahn, S., Memisevic, R. and Bengio, Y., 2017, February. Denoising criterion for variational auto-encoding framework. In Thirty-First AAAI Conference on Artificial Intelligence.\n",
    "\n",
    "[5] Rolinek, M., Zietlow, D. and Martius, G., 2019. Variational Autoencoders Pursue PCA Directions (by Accident). In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 12406-12415).\n",
    "\n",
    "[6] Lei, N., Luo, Z., Yau, S.T. and Gu, D.X., 2018. Geometric understanding of deep learning. arXiv preprint arXiv:1805.10451.\n",
    "\n",
    "[7] Kingma, D.P. and Welling, M., 2013. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
