{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hauptkomponentenanalyse vs. \n",
    "# Denoising Variational Autoencoders\n",
    "\n",
    "## _Intuition, Formalismus und Beispiele_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "jupyter nbconvert PCAvsDVAE.ipynb --to slides --post serve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Eine intuitive Perspektive ...\n",
    "\n",
    "#### \"... realistische, hochdimensionale Daten konzentrieren sich in der NÃ¤he einer nichtlinearen, niedrigdimensionalen Mannigfaltigkeit ...\"\n",
    "\n",
    "![](manifold.png)\n",
    "\n",
    "#### Aber wie lernt man die Mannigfaltigkeit und die Wahrscheinlichkeitsverteilung darauf?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](manifold-generic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hauptkomponentenanalyse \n",
    "# (Principal Component Analysis, PCA)\n",
    "* __unsupervised__ learning\n",
    "* __linear transformation__ \n",
    "    * \"encode\" a set of observations to a new coordinate system in which the values of the first coordinate (component) have the largest possible variance [2]\n",
    "    * the resulting coordinates (components) are uncorrelated with the preceeding coordinates\n",
    "* practically computing\n",
    "    * __eigendecomposition of the covariance matrix__\n",
    "    * __singular value decomposition__ of the observations\n",
    "* used for __dimensionality reduction__\n",
    "* __reconstructions of the observations__(\"decoding\") from the leading __principal components__ have the __least total squared error__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Grundlegende Mathematik der PCA\n",
    "\n",
    "* Let $\\{y_i\\}^N_{i=1}$ be a set of $N$ observations vectors, each of size $n$, with $n\\leq N$. \n",
    "\n",
    "* A __linear transformation__ on a finite dimensional vector can be expressed as a __matrix multiplication__: \n",
    "\n",
    "$$ \\begin{align} x_i = W y_i \\end{align} $$  \n",
    "  \n",
    "where $y_i \\in R^{n}, x_i \\in R^{m}$ and $W \\in R^{nxm}$. \n",
    "\n",
    "* Each $j-th$ element in $x_i$ is the __inner product__ between $y_i$ and the $j-th$ column in $W$, denoted as $w_j$. Let $Y \\in R^{nxN}$ be a matrix obtained by horizontally concatenating $\\{y_i\\}^N_{i=1}$, \n",
    "\n",
    "$$ Y = \\begin{bmatrix} | ... | \\\\ y_1 ... y_N \\\\ | ... | \\end{bmatrix} $$\n",
    "\n",
    "* Given the __linear transformation__, it is clear that:\n",
    "\n",
    "$$ X = W^TY,  X_0 = W^TY_0, $$\n",
    "\n",
    "where $Y_0$ is the matrix of centered (i.e. subtract the mean from each each observation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In particular, when $W^T$ represents the __transformation applying Principal Component Analysis__, we denote $W = P$. Each column of $P$, denoted $\\{p_j\\}^n_{j=1}$ is a __loading vector__, whereas each transformed vector $\\{x_i\\}^N_{i=1}$ is a __principal component__.\n",
    "\n",
    "* The first loading vector is the unit vector with which the inner products of the observations have the __greatest variance__:\n",
    "\n",
    "$$ \\max w_1^T Y_0Y_0^Tw_1, w_1^Tw_1 = 1$$\n",
    "\n",
    "* The solution of the previous equation is the first eigenvector of the __sample covariance matrix__ $Y_0Y_0^T$ corresponding to the largest eigenvalue.\n",
    "\n",
    "http://www.analytik.ethz.ch/vorlesungen/chemometrie/2_PCA_Monitor.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Autoencoders\n",
    "* unsupervised neural network\n",
    "* minimize the error of reconstructions of observations [1]\n",
    "* \n",
    "\n",
    "https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html\n",
    "\n",
    "\n",
    "# PCA vs. Autoencoders\n",
    "*  an autoencoder with a single fully-connected hidden layer, a linear activation function and a squared error cost function is closely related to PCA - its weights span the principal subspace [3]\n",
    "* \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Variational Autoencoders\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Denoising Variational Autoencoders\n",
    "\n",
    "https://github.com/dojoteef/dvae\n",
    "\n",
    "https://github.com/block98k/Denoise-VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# References and further reading\n",
    "[1] Ian Goodfellow, Yoshua Bengio and Aaron Courville, Deep Learning, MIT Press, 2016.\n",
    "\n",
    "[2] Jerome H. Friedman, Robert Tibshirani, and Trevor Hastie, The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Springer, 2017.\n",
    "\n",
    "[3] Plaut, E., 2018. From principal subspaces to principal components with linear autoencoders. arXiv preprint arXiv:1804.10253.\n",
    "\n",
    "[4] Im, D.I.J., Ahn, S., Memisevic, R. and Bengio, Y., 2017, February. Denoising criterion for variational auto-encoding framework. In Thirty-First AAAI Conference on Artificial Intelligence.\n",
    "\n",
    "[5] Rolinek, M., Zietlow, D. and Martius, G., 2019. Variational Autoencoders Pursue PCA Directions (by Accident). In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 12406-12415).\n",
    "\n",
    "[6] Lei, N., Luo, Z., Yau, S.T. and Gu, D.X., 2018. Geometric understanding of deep learning. arXiv preprint arXiv:1805.10451.\n",
    "\n",
    "[7] Kingma, D.P. and Welling, M., 2013. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
